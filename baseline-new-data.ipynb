{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EM shower reconstruction at SND@LHC\n",
    "\n",
    "1. __make sure the preprocessing has already been done__\n",
    "\n",
    "2. __make sure `results` folder exists__\n",
    "\n",
    "https://arxiv.org/pdf/2002.08722.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to JupyROOT 6.18/00\n"
     ]
    }
   ],
   "source": [
    "# imports from utils.py & net.py\n",
    "from utils import DataPreprocess, Parameters\n",
    "#from net import SNDNet, BNN, MyDataset, digitize_signal, digitize_signal_1d\n",
    "\n",
    "# python\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "from matplotlib import pylab as plt\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from IPython import display\n",
    "\n",
    "# system\n",
    "import os\n",
    "import gc  # Gabage collector interface (to debug stuff)\n",
    "import sys\n",
    "\n",
    "# ml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# dl\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA devices available:\n",
      "\tQuadro RTX 4000\twith CUDA capability (7, 5)\n",
      "\tQuadro RTX 4000\twith CUDA capability (7, 5)\n"
     ]
    }
   ],
   "source": [
    "# Test to see if cuda is available or not + listed the CUDA devices that are available\n",
    "try:\n",
    "    assert(torch.cuda.is_available())\n",
    "except:\n",
    "    raise Exception(\"CUDA is not available\")\n",
    "    \n",
    "n_devices = torch.cuda.device_count()\n",
    "print(\"CUDA devices available:\")\n",
    "\n",
    "for i in range(n_devices):\n",
    "    print(\"\\t{}\\twith CUDA capability {}\".format(torch.cuda.get_device_name      (device=i), \n",
    "                                                 torch.cuda.get_device_capability(device=i)))\n",
    "\n",
    "device = torch.device(\"cuda\", 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn off interactive plotting: for long run it screws up everything\n",
    "plt.ioff()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DETECTOR_PARAMS = Parameters(\"SNDatLHC\")\n",
    "DETECTOR_CONFIG = DETECTOR_PARAMS.snd_params[DETECTOR_PARAMS.configuration]\n",
    "\n",
    "# number of planes of the detector\n",
    "#NB_PLANE = dict()\n",
    "\n",
    "#NB_PLANE['scifi']   = len(DETECTOR_CONFIG['SciFi_tracker']        ['TT_POSITIONS'])\n",
    "#NB_PLANE['up_mu']   = len(DETECTOR_CONFIG['Mu_tracker_upstream']  ['TT_POSITIONS'])\n",
    "#NB_PLANE['down_mu'] = len(DETECTOR_CONFIG['Mu_tracker_downstream']['TT_POSITIONS'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data processing\n",
    "\n",
    "Here we load and process __pickle__ files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from process_pickle import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = dict()\n",
    "DATA_PATH['nuel']  = \"~/snd_data/nue\"\n",
    "DATA_PATH['numu']  = \"~/snd_data/numu\"\n",
    "DATA_PATH['nutau'] = \"~/snd_data/nutau\"\n",
    "\n",
    "EVENTS_PER_FILE = 4000 # todo -> read from the files ?\n",
    "FILES_NUM       = 10   # MAX=100 / todo -> read from directory ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scifi_arr, mu_arr, en_arr = load_dataframes(DETECTOR_PARAMS, \n",
    "#                                            DATA_PATH, EVENTS_PER_FILE, FILES_NUM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scifi_arr, mu_arr, en_arr = merge_events_arrays(scifi_arr, mu_arr, en_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#en_arr = normalise_target_energy(en_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation\n",
    "\n",
    "Here we prepare (load or, if needed, create) the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operate_datasets import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create_dataset('true', DETECTOR_PARAMS, DATA_PATH, EVENTS_PER_FILE, FILES_NUM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create_dataset('sum', DETECTOR_PARAMS, DATA_PATH, EVENTS_PER_FILE, FILES_NUM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create_dataset('longitudal', DETECTOR_PARAMS, DATA_PATH, EVENTS_PER_FILE, FILES_NUM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading the tt_cleared.pkl & y_cleared.pkl files by chunk of CCDIS and NueEElastic\n",
      "Before Reduction (file ~/snd_data/nue):\n",
      "  TT_df  : 40000\n",
      "  MU_df  : 40000\n",
      "  y_full : 40000\n",
      "Before Reduction (file ~/snd_data/numu):\n",
      "  TT_df  : 40000\n",
      "  MU_df  : 40000\n",
      "  y_full : 40000\n",
      "Before Reduction (file ~/snd_data/nutau):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/120000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  TT_df  : 40000\n",
      "  MU_df  : 40000\n",
      "  y_full : 40000\n",
      "After Reduction  :\n",
      "\n",
      "Particle type: nuel\n",
      "  scifi_arr : 40000\n",
      "  mu_arr    : 40000\n",
      "  en_arr    : 40000\n",
      "Particle type: numu\n",
      "  scifi_arr : 40000\n",
      "  mu_arr    : 40000\n",
      "  en_arr    : 40000\n",
      "Particle type: nutau\n",
      "  scifi_arr : 40000\n",
      "  mu_arr    : 40000\n",
      "  en_arr    : 40000\n",
      "\n",
      "combined_scifi_arr : 120000\n",
      "combined_mu_arr: 120000\n",
      "combined_en_arr: 120000\n",
      "projection\n",
      "(5, 201, 201) (5, 10, 1) (3, 67, 62)\n",
      "(5, 201) (5, 201)\n",
      "(5, 1) (5, 201)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "all the input array dimensions except for the concatenation axis must match exactly",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-995a628da6aa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcreate_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'projection'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDETECTOR_PARAMS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDATA_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEVENTS_PER_FILE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFILES_NUM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/SND_trackers_My_data/operate_datasets.py\u001b[0m in \u001b[0;36mcreate_dataset\u001b[0;34m(mode, detector_params, paths_dict, events_per_file, files_num, used_data_coef)\u001b[0m\n\u001b[1;32m    126\u001b[0m                                 \u001b[0mdetector_params\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m                                 \u001b[0mused_data_coef\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m                                 SGN_DGT_MODES[mode])\n\u001b[0m\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0msave_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_arr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_arr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_fname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/SND_trackers_My_data/operate_datasets.py\u001b[0m in \u001b[0;36mmake_dataset\u001b[0;34m(scifi_arr, mu_arr, en_arr, detector_params, used_data_coef, sgn_dgt_mode)\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mup_mu_x\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mup_mu_y\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m             \u001b[0mx_vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscifi_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mup_mu_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoen_mu_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m             \u001b[0my_vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscifi_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mup_mu_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoen_mu_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: all the input array dimensions except for the concatenation axis must match exactly"
     ]
    }
   ],
   "source": [
    "create_dataset('projection', DETECTOR_PARAMS, DATA_PATH, EVENTS_PER_FILE, FILES_NUM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# memory troubles!\n",
    "# be very carefull when using this\n",
    "### create_dataset('plane', DETECTOR_PARAMS, DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full_X, full_y = load_dataset('', 'longitudal')\n",
    "# full_X[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support vector regression baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_X, full_y = load_dataset('~/snd_data/new_dataset/', 'longitudal')\n",
    "min_clip = 25\n",
    "\n",
    "X_train, y_train = dataset_split(full_X, full_y)\n",
    "X_train, y_train = dataset_clip(X_train, y_train, min_clip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "reg_svr = svm.SVR(gamma='scale')\n",
    "#reg_svr = svm.LinearSVR(max_iter=10**5)\n",
    "\n",
    "reg_svr.fit(X_train, y_train)\n",
    "\n",
    "score_svr = reg_svr.score(X_train, y_train)\n",
    "\n",
    "print('SVM: ', score_svr)\n",
    "\n",
    "y_pred_svr = reg_svr.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_sum = X_train.sum(axis=1).reshape(-1,1)\n",
    "y_sum = y_train.reshape(-1,1)\n",
    "y_pred_svr = y_pred_svr.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_res_vs_energy(X_sum, y_sum, y_pred_svr) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_res_hist(y_sum, y_pred_svr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_2d_energy_hist(X_sum, y_sum, y_pred_svr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_scores(y_sum, y_pred_svr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frac_resol = np.divide((y_pred_svr - y_sum), y_sum)\n",
    "\n",
    "print((frac_resol < 0).sum())\n",
    "print((frac_resol >= 0).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "from pyro.nn import PyroModule, PyroSample\n",
    "from pyro.infer.autoguide import AutoDiagonalNormal\n",
    "from pyro.infer import SVI, Trace_ELBO, Predictive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_X, full_y = load_dataset('~/snd_data/new_dataset/', 'sum')\n",
    "min_clip = 25\n",
    "\n",
    "X_train, y_train = dataset_split(full_X, full_y)\n",
    "X_train, y_train = dataset_clip(X_train, y_train, min_clip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.tensor(X_train).float().reshape(-1,1)\n",
    "y_train = torch.tensor(y_train).float().reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyro.distributions import constraints\n",
    "\n",
    "\n",
    "class BayesianRegression(PyroModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = PyroModule[nn.Linear](1, 1)\n",
    "        \n",
    "        # prior over parameters\n",
    "        self.linear.weight = PyroSample(dist.Normal(1e-4, 5e-5).expand([1, 1]).to_event(2))\n",
    "        self.linear.bias   = PyroSample(dist.Normal(1e-1, 5e-2).expand([1]).to_event(1))\n",
    "        return\n",
    "    \n",
    "    def forward(self, x, y=None):\n",
    "        # energy (from the model)\n",
    "        mean = self.linear(x).squeeze(-1)\n",
    "        \n",
    "        # noise (learnable)\n",
    "        sigma = pyro.sample(\"sigma\", dist.Uniform(0., 1.)) \n",
    "        \n",
    "        with pyro.plate(\"data\", x.shape[0]):\n",
    "            obs = pyro.sample(\"obs\", dist.Normal(mean, sigma), obs=y)\n",
    "        \n",
    "        return mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(x, y=None):\n",
    "    bias   = pyro.sample(\"bias\",   dist.Normal(1e-4, 1e-4))\n",
    "    weight = pyro.sample(\"weight\", dist.Normal(0.1, 0.1))\n",
    "\n",
    "    mean = bias + weight * x # + mu_weight * muon_hits\n",
    "    mean = mean.squeeze(-1)\n",
    "    sigma = pyro.sample(\"sigma\", dist.Uniform(0., 0.1)) \n",
    "\n",
    "    with pyro.plate(\"data\", x.shape[0]):\n",
    "        pyro.sample(\"obs\", dist.Normal(mean, sigma), obs=y)\n",
    "\n",
    "    return mean\n",
    "\n",
    "\n",
    "def guide(x, y=None):\n",
    "    b_loc   = pyro.param(\"b_loc\",   torch.tensor(1e-4), constraint=constraints.positive)\n",
    "    b_scale = pyro.param(\"b_scale\", torch.tensor(1e-4), constraint=constraints.positive)\n",
    "\n",
    "    w_loc   = pyro.param(\"w_loc\",   torch.tensor(0.1), constraint=constraints.positive)\n",
    "    w_scale = pyro.param(\"w_scale\", torch.tensor(0.1), constraint=constraints.positive)\n",
    "\n",
    "    bias =   pyro.sample(\"bias\",   dist.Normal(b_loc, b_scale))\n",
    "    weight = pyro.sample(\"weight\", dist.Normal(w_loc, w_scale))\n",
    "\n",
    "    #sigma_loc = pyro.param('sigma_loc', torch.tensor(5.))\n",
    "    sigma = pyro.sample(\"sigma\", dist.Normal(0.05, 0.005))\n",
    "\n",
    "    mean = bias + weight * x\n",
    "    \n",
    "    return mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reg_model = model # BayesianRegression()\n",
    "reg_guide = guide # AutoDiagonalNormal(model)\n",
    "\n",
    "num_steps = 50\n",
    "initial_lr = 1.0\n",
    "gamma = 0.5  # final learning rate will be gamma * initial_lr\n",
    "lrd = gamma ** (1 / num_steps)\n",
    "\n",
    "adam = pyro.optim.ClippedAdam({\"lr\": initial_lr, 'lrd': lrd})\n",
    "\n",
    "#adam = pyro.optim.ClippedAdam({\"lr\": 1.0, \"lrd\": 0.5})\n",
    "svi = SVI(reg_model, reg_guide, adam, loss=Trace_ELBO())\n",
    "num_iterations = 200\n",
    "\n",
    "pyro.clear_param_store()\n",
    "loss_arr = []\n",
    "\n",
    "\n",
    "for j in range(num_iterations):\n",
    "    loss = svi.step(X_train, y_train)\n",
    "\n",
    "    loss_arr.append(loss)\n",
    "        \n",
    "    if j % 25 == 0:\n",
    "        print(\"[iteration %04d] loss: %.4f\" % (j + 1, loss / len(X_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_arr)\n",
    "plt.yscale('log')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('ELBO loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#guide.requires_grad_(False)\n",
    "\n",
    "for name, value in pyro.get_param_store().items():\n",
    "    print(name, pyro.param(name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summary(samples):\n",
    "    site_stats = {}\n",
    "    for site_name, values in samples.items():\n",
    "        site_stats[site_name] = {\n",
    "            \"mean\": torch.mean(values, 0),\n",
    "            \"std\" : torch.std (values, 0),\n",
    "            \"5%\"  : values.kthvalue(int(len(values) * 0.05), dim=0)[0],\n",
    "            \"95%\" : values.kthvalue(int(len(values) * 0.95), dim=0)[0],\n",
    "        }\n",
    "        \n",
    "    return site_stats\n",
    "\n",
    "\n",
    "predictive = Predictive(model, guide=guide, num_samples=800)\n",
    "samples = predictive(X_train)\n",
    "pred_summary = summary(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = pred_summary[\"obs\"]\n",
    "y = pred_summary[\"obs\"]\n",
    "predictions = pd.DataFrame({\n",
    "    \"mu_mean\": mu[\"mean\"],\n",
    "    \"mu_perc_5\": mu[\"5%\"],\n",
    "    \"mu_perc_95\": mu[\"95%\"],\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(X_train[:,0], predictions['mu_mean'].to_numpy(), 'o')\n",
    "plt.plot(X_train[:,0], predictions['mu_perc_5'].to_numpy(), 'o')\n",
    "plt.plot(X_train[:,0], predictions['mu_perc_95'].to_numpy(), 'o')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions['mu_mean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_2d_energy_hist(X_arr, y_true, predictions):\n",
    "    fig, ax = plt.subplots(figsize=(8,6))\n",
    "\n",
    "    y_pred_min  = predictions[\"mu_perc_5\"].to_numpy()\n",
    "    y_pred_mean = predictions[\"mu_mean\"]  .to_numpy()\n",
    "    y_pred_max  = predictions[\"mu_perc_95\"].to_numpy()\n",
    "    \n",
    "    print(X_arr.shape)\n",
    "    print(y_true.shape)\n",
    "    \n",
    "    hist = ax.hist2d(X_arr[:,0].numpy(), y_true.numpy(), \n",
    "                     bins=100, norm=mpl.colors.LogNorm(), vmax=150)\n",
    "    \n",
    "    plt.ylim(0)\n",
    "    plt.xlabel('pixel sum')\n",
    "    plt.ylabel('normalised energy')\n",
    "\n",
    "    #plt.axvline(x=min_clip, c='m', alpha=0.9, label='Min clip ' + str(min_clip))\n",
    "    ax.plot(X_arr[:, 0], y_pred_mean, 'g.', alpha=0.3, label='L2 w. clipped data')\n",
    "    ax.plot(X_arr[:, 0], y_pred_min, 'r.', marker='.', alpha=0.3, label='5%')\n",
    "    ax.plot(X_arr[:, 0], y_pred_max, 'b.', marker='.', alpha=0.3, label='95%')\n",
    "    #ax.fill_between(X_arr[:, 0], y_pred_min, y_pred_max,  alpha=0.3, color='deeppink')\n",
    "\n",
    "    #cbar = fig.colorbar(hist[3], ax=ax)\n",
    "    #|cbar.set_label('# of particles')\n",
    "\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_2d_energy_hist(X_train, y_train, predictions) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear regression on true hits with muon detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_one_dataframe(params, path_nu):\n",
    "    proc_path_nu  = os.path.expandvars(path_nu)\n",
    "        \n",
    "    step_size = 4000                   # events in one file\n",
    "    files_num = 100                    # number of files\n",
    "    file_size = files_num * step_size  # total number of events\n",
    "    \n",
    "    reidx_TT_df, reidx_y_full = read_chunklist(path_nu, step_size, file_size)\n",
    "    \n",
    "    return reidx_TT_df, reidx_y_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operate_datasets import *\n",
    "\n",
    "def create_nu_dataset(mode, detector_params, path_nu, used_data_coef = 1.0):\n",
    "    \n",
    "    X_arr, y_arr = load_one_dataframe(detector_params, path_nu)\n",
    "        \n",
    "    dataset_fname = 'big_nue_dataset.npz'\n",
    "\n",
    "    full_X, full_y = make_dataset(X_arr, y_arr, detector_params,\n",
    "                                  used_data_coef = used_data_coef,\n",
    "                                  sgn_dgt_mode = SGN_DGT_MODES[mode])\n",
    "\n",
    "    save_dataset(full_X, full_y, dataset_fname)\n",
    "        \n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_nu_dataset('sum', DETECTOR_PARAMS, PATH_NUEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(path, mode='sum'):\n",
    "    assert mode in SGN_DGT_MODE_NAMES\n",
    "    \n",
    "    dataset_fname = 'big_nue_dataset.npz'\n",
    "\n",
    "    full_X, full_y = None, None\n",
    "\n",
    "    # load if exists\n",
    "    full_dts = None\n",
    "\n",
    "    with open(dataset_fname, 'rb') as file:\n",
    "        full_dts = np.load(file)#, allow_pickle=True)\n",
    "\n",
    "        full_X = full_dts['x']\n",
    "        full_y = full_dts['y']\n",
    "        \n",
    "    return full_X, full_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_X, full_y = load_dataset('', 'sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_X"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
