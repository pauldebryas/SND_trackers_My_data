{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EM shower reconstruction with the SciFi at SND@LHC\n",
    "\n",
    "1. __make sure the preprocessing has already been done__\n",
    "\n",
    "2. __make sure `results` folder exists__\n",
    "\n",
    "\n",
    "based on https://github.com/pauldebryas/SND_trackers_My_data/tree/SNDatLHC_neutrino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to JupyROOT 6.18/00\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# Import Class from utils.py & net.py file\n",
    "from utils import DataPreprocess, Parameters\n",
    "from net import SNDNet, MyDataset, digitize_signal\n",
    "\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pylab as plt\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from IPython import display\n",
    "\n",
    "import os\n",
    "import gc  # Gabage collector interface (to debug stuff)\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Welcome!\n",
      "\n",
      "CUDA devices available:\n",
      "\n",
      "\tQuadro P2200\twith CUDA capability (6, 1)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test to see if cuda is available or not + listed the CUDA devices that are available\n",
    "try:\n",
    "    assert(torch.cuda.is_available())\n",
    "except:\n",
    "    raise Exception(\"CUDA is not available\")\n",
    "    \n",
    "n_devices = torch.cuda.device_count()\n",
    "print(\"\\nWelcome!\\n\\nCUDA devices available:\\n\")\n",
    "\n",
    "for i in range(n_devices):\n",
    "    print(\"\\t{}\\twith CUDA capability {}\".format(torch.cuda.get_device_name      (device=i), \n",
    "                                                 torch.cuda.get_device_capability(device=i)))\n",
    "print(\"\\n\")\n",
    "\n",
    "device = torch.device(\"cuda\", 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn off interactive plotting: for long run it screws up everything\n",
    "plt.ioff()\n",
    "\n",
    "# Here we choose the geometry with 9 time the radiation length\n",
    "params = Parameters(\"SNDatLHC\")  #!!!!!!!!!!!!!!!!!!!!!CHANGE THE DIMENTION !!!!!!!!!!!!!!!!\n",
    "processed_file_path_1 = os.path.expandvars(\"$HOME/snd_data/processed_data/CCDIS\")\n",
    "processed_file_path_2 = os.path.expandvars(\"$HOME/snd_data/processed_data/NuEElastic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading the tt_cleared.pkl & y_cleared.pkl files by chunk of CCDIS and NueEElastic\n"
     ]
    }
   ],
   "source": [
    "# --- LOAD THE reindex_TT_df & reindex_y_full PD.DATAFRAME ---\n",
    "\n",
    "chunklist_TT_df_CCDIS = []  # list of the TT_df file of each chunk\n",
    "chunklist_TT_df_NueEElastic = []\n",
    "chunklist_y_full_CCDIS = [] # list of the y_full file of each chunk\n",
    "chunklist_y_full_NueEElastic = []\n",
    "\n",
    "# It is reading and analysing data by chunk instead of all at the time (memory leak problem)\n",
    "print(\"\\nReading the tt_cleared.pkl & y_cleared.pkl files by chunk of CCDIS and NueEElastic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 398/398 [00:06<00:00, 57.45it/s]\n"
     ]
    }
   ],
   "source": [
    "# CCDIS\n",
    "\n",
    "step_size = 1000    # size of a chunk\n",
    "file_size = 400000  # size of the CCDIS BigFile\n",
    "n_steps = int(file_size / step_size) # number of chunks\n",
    "\n",
    "# First 2 \n",
    "outpath_1 = processed_file_path_1 + \"/{}\".format(0)\n",
    "chunklist_TT_df_CCDIS.append(pd.read_pickle(os.path.join(outpath_1, \"tt_cleared.pkl\")))\n",
    "chunklist_y_full_CCDIS.append(pd.read_pickle(os.path.join(outpath_1, \"y_cleared.pkl\")))\n",
    "\n",
    "outpath_1 = processed_file_path_1 + \"/{}\".format(1)\n",
    "chunklist_TT_df_CCDIS.append(pd.read_pickle(os.path.join(outpath_1, \"tt_cleared.pkl\")))\n",
    "chunklist_y_full_CCDIS.append(pd.read_pickle(os.path.join(outpath_1, \"y_cleared.pkl\")))\n",
    "\n",
    "reindex_TT_df_CCDIS = pd.concat([chunklist_TT_df_CCDIS[0],chunklist_TT_df_CCDIS[1]],ignore_index=True)\n",
    "reindex_y_full_CCDIS = pd.concat([chunklist_y_full_CCDIS[0],chunklist_y_full_CCDIS[1]], ignore_index=True)\n",
    "\n",
    "for i in tqdm(range(n_steps-2)):  # tqdm: make your loops show a progress bar in terminal\n",
    "    outpath_1 = processed_file_path_1 + \"/{}\".format(i+2)\n",
    "    # add all the tt_cleared.pkl files read_pickle and add to the chunklist_TT_df list\n",
    "    chunklist_TT_df_CCDIS.append(pd.read_pickle(os.path.join(outpath_1, \"tt_cleared.pkl\")))\n",
    "    # add all the y_cleared.pkl files read_pickle and add to the chunklist_y_full list\n",
    "    chunklist_y_full_CCDIS.append(pd.read_pickle(os.path.join(outpath_1, \"y_cleared.pkl\")))\n",
    "    reindex_TT_df_CCDIS = pd.concat([reindex_TT_df_CCDIS,chunklist_TT_df_CCDIS[i+2]], ignore_index=True)\n",
    "    reindex_y_full_CCDIS = pd.concat([reindex_y_full_CCDIS,chunklist_y_full_CCDIS[i+2]], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 397/397 [00:03<00:00, 100.35it/s]\n"
     ]
    }
   ],
   "source": [
    "#NueEElastic\n",
    "\n",
    "step_size = 1000    # size of a chunk\n",
    "file_size = 399000  # size of the NueEElastic BigFile\n",
    "n_steps = int(file_size / step_size) # number of chunks\n",
    "\n",
    "#First 2 \n",
    "outpath_2 = processed_file_path_2 + \"/{}\".format(0)\n",
    "chunklist_TT_df_NueEElastic.append(pd.read_pickle(os.path.join(outpath_2, \"tt_cleared.pkl\")))\n",
    "chunklist_y_full_NueEElastic.append(pd.read_pickle(os.path.join(outpath_2, \"y_cleared.pkl\")))\n",
    "\n",
    "outpath_2 = processed_file_path_2 + \"/{}\".format(1)\n",
    "chunklist_TT_df_NueEElastic.append(pd.read_pickle(os.path.join(outpath_2, \"tt_cleared.pkl\")))\n",
    "chunklist_y_full_NueEElastic.append(pd.read_pickle(os.path.join(outpath_2, \"y_cleared.pkl\")))\n",
    "\n",
    "reindex_TT_df_NueEElastic = pd.concat([chunklist_TT_df_NueEElastic[0],chunklist_TT_df_NueEElastic[1]],ignore_index=True)\n",
    "reindex_y_full_NueEElastic = pd.concat([chunklist_y_full_NueEElastic[0],chunklist_y_full_NueEElastic[1]], ignore_index=True)\n",
    "\n",
    "for i in tqdm(range(n_steps-2)):  # tqdm: make your loops show a progress bar in terminal\n",
    "    outpath_2 = processed_file_path_2 + \"/{}\".format(i+2)\n",
    "    chunklist_TT_df_NueEElastic.append(pd.read_pickle(os.path.join(outpath_2, \"tt_cleared.pkl\"))) # add all the tt_cleared.pkl files read_pickle and add to the chunklist_TT_df list\n",
    "    chunklist_y_full_NueEElastic.append(pd.read_pickle(os.path.join(outpath_2, \"y_cleared.pkl\"))) # add all the y_cleared.pkl files read_pickle and add to the chunklist_y_full list\n",
    "    reindex_TT_df_NueEElastic = pd.concat([reindex_TT_df_NueEElastic,chunklist_TT_df_NueEElastic[i+2]], ignore_index=True)\n",
    "    reindex_y_full_NueEElastic = pd.concat([reindex_y_full_NueEElastic,chunklist_y_full_NueEElastic[i+2]], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nprint(\"Before Reduction  :\")\\nprint(\"TT_df inelastic: \" + str(len(reindex_TT_df_CCDIS)))\\nprint(\"y_full inelastic: \" + str(len(reindex_y_full_CCDIS)))\\nprint(\"TT_df elastic: \" + str(len(reindex_TT_df_NueEElastic)))\\nprint(\"y_full elastic: \" + str(len(reindex_y_full_NueEElastic)))\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "print(\"Before Reduction  :\")\n",
    "print(\"TT_df inelastic: \" + str(len(reindex_TT_df_CCDIS)))\n",
    "print(\"y_full inelastic: \" + str(len(reindex_y_full_CCDIS)))\n",
    "print(\"TT_df elastic: \" + str(len(reindex_TT_df_NueEElastic)))\n",
    "print(\"y_full elastic: \" + str(len(reindex_y_full_NueEElastic)))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting events to ensure equal number of elastic and inelastic events\n",
    "event_limit = min(len(reindex_TT_df_CCDIS),len(reindex_TT_df_NueEElastic))\n",
    "\n",
    "remove = int(len(reindex_TT_df_CCDIS)-event_limit)+1\n",
    "reindex_TT_df_CCDIS = reindex_TT_df_CCDIS[:-remove]\n",
    "reindex_y_full_CCDIS = reindex_y_full_CCDIS[:-remove]\n",
    "\n",
    "remove = int(len(reindex_TT_df_NueEElastic)-event_limit)+1\n",
    "reindex_TT_df_NueEElastic = reindex_TT_df_NueEElastic[:-remove]\n",
    "reindex_y_full_NueEElastic = reindex_y_full_NueEElastic[:-remove]\n",
    "\n",
    "# Merging CCDIS and NueEElastic in a single array\n",
    "reindex_TT_df = pd.concat([reindex_TT_df_CCDIS,reindex_TT_df_NueEElastic], ignore_index=True)\n",
    "reindex_y_full = pd.concat([reindex_y_full_CCDIS,reindex_y_full_NueEElastic], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# -1 for CCDIS\\nfor i in range(event_limit-1):\\n    reindex_y_full.iloc[i][\"Label\"] = -1  \\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "print(\"After Reduction  :\")\n",
    "print(\"TT_df inelastic: \" + str(len(reindex_TT_df_CCDIS)))\n",
    "print(\"y_full inelastic: \" + str(len(reindex_y_full_CCDIS)))\n",
    "print(\"TT_df elastic: \" + str(len(reindex_TT_df_NueEElastic)))\n",
    "print(\"y_full elastic: \" + str(len(reindex_y_full_NueEElastic)))\n",
    "\n",
    "print(\"Combined TT_df : \" + str(len(reindex_TT_df)))\n",
    "print(\"Combined y_full: \" + str(len(reindex_y_full)))\n",
    "'''\n",
    "'''\n",
    "# -1 for CCDIS\n",
    "for i in range(event_limit-1):\n",
    "    reindex_y_full.iloc[i][\"Label\"] = -1  \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Splitting the data into a training and a testing sample\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fsergeev/anaconda3/envs/ship_tt/lib/python3.7/site-packages/ipykernel_launcher.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "# reset to empty space\n",
    "chunklist_TT_df_NueEElastic = []\n",
    "chunklist_y_full_NueEElastic = []\n",
    "reindex_TT_df_NueEElastic = []\n",
    "reindex_y_full_NueEElastic = []\n",
    "\n",
    "chunklist_TT_df_CCDIS = []\n",
    "chunklist_y_full_CCDIS = []\n",
    "reindex_TT_df_CCDIS = []\n",
    "reindex_y_full_CCDIS = []\n",
    "\n",
    "\n",
    "nb_of_plane = len(params.snd_params[params.configuration][\"TT_POSITIONS\"])\n",
    "\n",
    "# True value of NRJ for each true Nue event\n",
    "y = reindex_y_full[[\"E\"]]\n",
    "NORM = 1. / 4000\n",
    "y[\"E\"] *= NORM\n",
    "\n",
    "# Spliting\n",
    "print(\"\\nSplitting the data into a training and a testing sample\")\n",
    "\n",
    "indeces = np.arange(len(reindex_TT_df))\n",
    "train_indeces, test_indeces, _, _ = train_test_split(indeces, indeces, train_size=0.9, random_state=1543)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get smaller datasets to make training quicker\n",
    "\n",
    "debug_coef = 0.2\n",
    "\n",
    "train_indeces = train_indeces[0 : int(train_indeces.shape[0] * debug_coef)]\n",
    "test_indeces  = train_indeces[0 : int(test_indeces .shape[0] * debug_coef)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#batch_size = 512\n",
    "batch_size = 128\n",
    "\n",
    "train_dataset = MyDataset(reindex_TT_df, y, params, train_indeces, n_filters=nb_of_plane)\n",
    "train_batch_gen = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "\n",
    "test_dataset = MyDataset(reindex_TT_df, y, params, test_indeces, n_filters=nb_of_plane)\n",
    "test_batch_gen = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "# reset to empty space\n",
    "reindex_TT_df=[]\n",
    "\n",
    "# Saving the true Energy for the test sample\n",
    "TrueE_test=y[\"E\"][test_indeces]\n",
    "np.save(\"results/TrueE_test.npy\",TrueE_test)\n",
    "\n",
    "# Saving the Label for the test sample\n",
    "TrueLabel_test = reindex_y_full[\"Label\"][test_indeces]\n",
    "np.save(\"results/TrueLabel_test.npy\",TrueLabel_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 201, 201])\n"
     ]
    }
   ],
   "source": [
    "for X_batch, y_batch in train_batch_gen:\n",
    "    print(X_batch[0].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pyro\n",
    "#import pyro.distributions as dist\n",
    "#from pyro.nn import PyroModule, PyroSample\n",
    "#from pyro.infer.autoguide import AutoDiagonalNormal\n",
    "#from pyro.infer import SVI, Trace_ELBO, Predictive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘results/9X0_file’: File exists\n"
     ]
    }
   ],
   "source": [
    "# Creating the network\n",
    "net = SNDNet(n_input_filters=nb_of_plane).to(device)\n",
    "\n",
    "# Loose rate, num epoch and weight decay parameters of our network backprop actions\n",
    "lr = 1e-3\n",
    "opt = torch.optim.Adam(net.model.parameters(), lr=lr, weight_decay=0.01)\n",
    "num_epochs = 3\n",
    "\n",
    "train_loss = []\n",
    "val_accuracy_1 = []\n",
    "val_accuracy_2 = []\n",
    "\n",
    "# Create a directory where to store the 9X0 files\n",
    "os.system(\"mkdir results/9X0_file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Now Trainig the network:\n"
     ]
    }
   ],
   "source": [
    "#Training\n",
    "print(\"\\nNow Trainig the network:\")\n",
    "# Create a .txt file where we will store some info for graphs\n",
    "f=open(\"results/NN_performance.txt\",\"a\")\n",
    "f.write(\"Epoch/Time it took (s)/Loss/Validation energy (%)/Validation distance (%)\\n\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Logger(object):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def plot_losses(self, epoch, num_epochs, start_time):\n",
    "        # Print and save in NN_performance.txt the results for this epoch:\n",
    "        print(\"Epoch {} of {} took {:.3f}s\".format(epoch + 1, num_epochs, time.time() - start_time))\n",
    "        print(\"  training loss (in-iteration): \\t{:.6f}\".format(train_loss[-1]))\n",
    "        print(\"  validation Energy:\\t\\t{:.4f} %\".format(val_accuracy_1[-1]))\n",
    "        #print(\"  validation distance:\\t\\t{:.4f} %\".format(val_accuracy_2[-1]))\n",
    "\n",
    "        f=open(\"results/NN_performance.txt\",\"a\")\n",
    "        f.write(\"{};{:.3f};\".format(epoch + 1, time.time() - start_time))\n",
    "        f.write(\"\\t{:.6f};\".format(train_loss[-1]))\n",
    "        f.write(\"\\t\\t{:.4f}\\n\".format(val_accuracy_1[-1]))\n",
    "        #f.write(\"\\t\\t{:.4f}\\n\".format(val_accuracy_2[-1]))\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training(lr, num_epochs, opt):\n",
    "    try:\n",
    "        total = int(len(train_indeces) / batch_size)\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            # In each epoch, we do a full pass over the training data:\n",
    "            start_time = time.time()\n",
    "            net.model.train(True)\n",
    "            epoch_loss = 0\n",
    "            for X_batch, y_batch in tqdm(train_batch_gen, total=total):\n",
    "            # for X_batch, y_batch in train_batch_gen:\n",
    "                # train on batch\n",
    "                loss = net.compute_loss(X_batch, y_batch)\n",
    "                loss.backward()\n",
    "                opt.step()\n",
    "                opt.zero_grad()\n",
    "                epoch_loss += loss.item()\n",
    "\n",
    "            train_loss.append(epoch_loss / (len(train_indeces) // batch_size + 1))\n",
    "\n",
    "            y_score = []\n",
    "            with torch.no_grad():\n",
    "                for (X_batch, y_batch) in tqdm(test_batch_gen, total = int(len(test_indeces) / batch_size)):\n",
    "                    logits = net.predict(X_batch)\n",
    "                    y_pred = logits.cpu().detach().numpy()\n",
    "                    y_score.extend(y_pred)\n",
    "\n",
    "            y_score = mean_squared_error(y.iloc[test_indeces], np.asarray(y_score), multioutput='raw_values')\n",
    "            val_accuracy_1.append(y_score[0])\n",
    "            #val_accuracy_2.append(y_score[1])    \n",
    "\n",
    "            # Visualize\n",
    "            display.clear_output(wait=True)\n",
    "            logger.plot_losses(epoch, num_epochs, start_time)\n",
    "\n",
    "            #Saving network for each 10 epoch\n",
    "            if (epoch + 1) % 10 == 0:\n",
    "                with open(\"results/9X0_file/\" + str(epoch) + \"_9X0_coordconv.pt\", 'wb') as f:\n",
    "                    torch.save(net, f)       \n",
    "                lr = lr / 2\n",
    "                opt = torch.optim.Adam(net.model.parameters(), lr=lr)\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 of 3 took 75.452s\n",
      "  training loss (in-iteration): \t0.035781\n",
      "  validation Energy:\t\t0.0295 %\n"
     ]
    }
   ],
   "source": [
    "logger = Logger()\n",
    "run_training(lr, num_epochs, opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fsergeev/anaconda3/envs/ship_tt/lib/python3.7/site-packages/torch/serialization.py:656: SourceChangeWarning: source code of class 'torch.nn.modules.container.Sequential' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/home/fsergeev/anaconda3/envs/ship_tt/lib/python3.7/site-packages/torch/serialization.py:656: SourceChangeWarning: source code of class 'torch.nn.modules.conv.Conv2d' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/home/fsergeev/anaconda3/envs/ship_tt/lib/python3.7/site-packages/torch/serialization.py:656: SourceChangeWarning: source code of class 'torch.nn.modules.pooling.MaxPool2d' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/home/fsergeev/anaconda3/envs/ship_tt/lib/python3.7/site-packages/torch/serialization.py:656: SourceChangeWarning: source code of class 'torch.nn.modules.batchnorm.BatchNorm2d' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/home/fsergeev/anaconda3/envs/ship_tt/lib/python3.7/site-packages/torch/serialization.py:656: SourceChangeWarning: source code of class 'torch.nn.modules.activation.ReLU' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/home/fsergeev/anaconda3/envs/ship_tt/lib/python3.7/site-packages/torch/serialization.py:656: SourceChangeWarning: source code of class 'torch.nn.modules.linear.Linear' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save Prediction for epoch 9\n",
      "Save Prediction for epoch 19\n",
      "Save Prediction for epoch 29\n",
      "Save Prediction for epoch 39\n",
      "Save Prediction for epoch 49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘results/PredE_file’: File exists\n"
     ]
    }
   ],
   "source": [
    "# Saving the prediction at each epoch\n",
    "\n",
    "# Create a directory where to store the prediction files\n",
    "os.system(\"mkdir results/PredE_file\")\n",
    "\n",
    "for i in [9, 19, 29, 39, 49]:\n",
    "    net = torch.load(\"results/9X0_file/\" + str(i) + \"_9X0_coordconv.pt\")\n",
    "    preds = []\n",
    "    with torch.no_grad():\n",
    "        for (X_batch, y_batch) in test_batch_gen:\n",
    "            preds.append(net.predict(X_batch))\n",
    "    ans = np.concatenate([p.detach().cpu().numpy() for p in preds])\n",
    "    np.save(\"results/PredE_file/\" + str(i) + \"_PredE_test.npy\",ans[:, 0])\n",
    "    print(\"Save Prediction for epoch \"+ str(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
