{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EM shower reconstruction with the SciFi at SND@LHC\n",
    "\n",
    "1. __make sure the preprocessing has already been done__\n",
    "\n",
    "2. __make sure `results` folder exists__\n",
    "\n",
    "\n",
    "based on https://github.com/pauldebryas/SND_trackers_My_data/tree/SNDatLHC_neutrino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to JupyROOT 6.18/00\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# Import Class from utils.py & net.py file\n",
    "from utils import DataPreprocess, Parameters\n",
    "from net import SNDNet, BNN, MyDataset, digitize_signal\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pylab as plt\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from IPython import display\n",
    "\n",
    "import os\n",
    "import gc  # Gabage collector interface (to debug stuff)\n",
    "import sys\n",
    "\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "from pyro.nn import PyroModule, PyroSample\n",
    "from pyro.infer.autoguide import AutoDiagonalNormal\n",
    "from pyro.infer import SVI, Trace_ELBO, Predictive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Welcome!\n",
      "\n",
      "CUDA devices available:\n",
      "\n",
      "\tQuadro P2200\twith CUDA capability (6, 1)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test to see if cuda is available or not + listed the CUDA devices that are available\n",
    "try:\n",
    "    assert(torch.cuda.is_available())\n",
    "except:\n",
    "    raise Exception(\"CUDA is not available\")\n",
    "    \n",
    "n_devices = torch.cuda.device_count()\n",
    "print(\"\\nWelcome!\\n\\nCUDA devices available:\\n\")\n",
    "\n",
    "for i in range(n_devices):\n",
    "    print(\"\\t{}\\twith CUDA capability {}\".format(torch.cuda.get_device_name      (device=i), \n",
    "                                                 torch.cuda.get_device_capability(device=i)))\n",
    "print(\"\\n\")\n",
    "\n",
    "device = torch.device(\"cuda\", 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn off interactive plotting: for long run it screws up everything\n",
    "plt.ioff()\n",
    "\n",
    "# Here we choose the geometry with 9 time the radiation length\n",
    "params = Parameters(\"SNDatLHC\")  #!!!!!!!!!!!!!!!!!!!!!CHANGE THE DIMENTION !!!!!!!!!!!!!!!!\n",
    "processed_file_path_1 = os.path.expandvars(\"$HOME/snd_data/processed_data/CCDIS\")\n",
    "processed_file_path_2 = os.path.expandvars(\"$HOME/snd_data/processed_data/NuEElastic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading the tt_cleared.pkl & y_cleared.pkl files by chunk of CCDIS and NueEElastic\n"
     ]
    }
   ],
   "source": [
    "# --- LOAD THE reindex_TT_df & reindex_y_full PD.DATAFRAME ---\n",
    "\n",
    "chunklist_TT_df_CCDIS = []  # list of the TT_df file of each chunk\n",
    "chunklist_TT_df_NueEElastic = []\n",
    "chunklist_y_full_CCDIS = [] # list of the y_full file of each chunk\n",
    "chunklist_y_full_NueEElastic = []\n",
    "\n",
    "# It is reading and analysing data by chunk instead of all at the time (memory leak problem)\n",
    "print(\"\\nReading the tt_cleared.pkl & y_cleared.pkl files by chunk of CCDIS and NueEElastic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 398/398 [00:06<00:00, 57.47it/s]\n"
     ]
    }
   ],
   "source": [
    "# CCDIS\n",
    "\n",
    "step_size = 1000    # size of a chunk\n",
    "file_size = 400000  # size of the CCDIS BigFile\n",
    "n_steps = int(file_size / step_size) # number of chunks\n",
    "\n",
    "# First 2 \n",
    "outpath_1 = processed_file_path_1 + \"/{}\".format(0)\n",
    "chunklist_TT_df_CCDIS.append(pd.read_pickle(os.path.join(outpath_1, \"tt_cleared.pkl\")))\n",
    "chunklist_y_full_CCDIS.append(pd.read_pickle(os.path.join(outpath_1, \"y_cleared.pkl\")))\n",
    "\n",
    "outpath_1 = processed_file_path_1 + \"/{}\".format(1)\n",
    "chunklist_TT_df_CCDIS.append(pd.read_pickle(os.path.join(outpath_1, \"tt_cleared.pkl\")))\n",
    "chunklist_y_full_CCDIS.append(pd.read_pickle(os.path.join(outpath_1, \"y_cleared.pkl\")))\n",
    "\n",
    "reindex_TT_df_CCDIS = pd.concat([chunklist_TT_df_CCDIS[0],chunklist_TT_df_CCDIS[1]],ignore_index=True)\n",
    "reindex_y_full_CCDIS = pd.concat([chunklist_y_full_CCDIS[0],chunklist_y_full_CCDIS[1]], ignore_index=True)\n",
    "\n",
    "for i in tqdm(range(n_steps-2)):  # tqdm: make your loops show a progress bar in terminal\n",
    "    outpath_1 = processed_file_path_1 + \"/{}\".format(i+2)\n",
    "    # add all the tt_cleared.pkl files read_pickle and add to the chunklist_TT_df list\n",
    "    chunklist_TT_df_CCDIS.append(pd.read_pickle(os.path.join(outpath_1, \"tt_cleared.pkl\")))\n",
    "    # add all the y_cleared.pkl files read_pickle and add to the chunklist_y_full list\n",
    "    chunklist_y_full_CCDIS.append(pd.read_pickle(os.path.join(outpath_1, \"y_cleared.pkl\")))\n",
    "    reindex_TT_df_CCDIS = pd.concat([reindex_TT_df_CCDIS,chunklist_TT_df_CCDIS[i+2]], ignore_index=True)\n",
    "    reindex_y_full_CCDIS = pd.concat([reindex_y_full_CCDIS,chunklist_y_full_CCDIS[i+2]], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 397/397 [00:04<00:00, 98.20it/s] \n"
     ]
    }
   ],
   "source": [
    "#NueEElastic\n",
    "\n",
    "step_size = 1000    # size of a chunk\n",
    "file_size = 399000  # size of the NueEElastic BigFile\n",
    "n_steps = int(file_size / step_size) # number of chunks\n",
    "\n",
    "#First 2 \n",
    "outpath_2 = processed_file_path_2 + \"/{}\".format(0)\n",
    "chunklist_TT_df_NueEElastic.append(pd.read_pickle(os.path.join(outpath_2, \"tt_cleared.pkl\")))\n",
    "chunklist_y_full_NueEElastic.append(pd.read_pickle(os.path.join(outpath_2, \"y_cleared.pkl\")))\n",
    "\n",
    "outpath_2 = processed_file_path_2 + \"/{}\".format(1)\n",
    "chunklist_TT_df_NueEElastic.append(pd.read_pickle(os.path.join(outpath_2, \"tt_cleared.pkl\")))\n",
    "chunklist_y_full_NueEElastic.append(pd.read_pickle(os.path.join(outpath_2, \"y_cleared.pkl\")))\n",
    "\n",
    "reindex_TT_df_NueEElastic = pd.concat([chunklist_TT_df_NueEElastic[0],chunklist_TT_df_NueEElastic[1]],ignore_index=True)\n",
    "reindex_y_full_NueEElastic = pd.concat([chunklist_y_full_NueEElastic[0],chunklist_y_full_NueEElastic[1]], ignore_index=True)\n",
    "\n",
    "for i in tqdm(range(n_steps-2)):  # tqdm: make your loops show a progress bar in terminal\n",
    "    outpath_2 = processed_file_path_2 + \"/{}\".format(i+2)\n",
    "    chunklist_TT_df_NueEElastic.append(pd.read_pickle(os.path.join(outpath_2, \"tt_cleared.pkl\"))) # add all the tt_cleared.pkl files read_pickle and add to the chunklist_TT_df list\n",
    "    chunklist_y_full_NueEElastic.append(pd.read_pickle(os.path.join(outpath_2, \"y_cleared.pkl\"))) # add all the y_cleared.pkl files read_pickle and add to the chunklist_y_full list\n",
    "    reindex_TT_df_NueEElastic = pd.concat([reindex_TT_df_NueEElastic,chunklist_TT_df_NueEElastic[i+2]], ignore_index=True)\n",
    "    reindex_y_full_NueEElastic = pd.concat([reindex_y_full_NueEElastic,chunklist_y_full_NueEElastic[i+2]], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nprint(\"Before Reduction  :\")\\nprint(\"TT_df inelastic: \" + str(len(reindex_TT_df_CCDIS)))\\nprint(\"y_full inelastic: \" + str(len(reindex_y_full_CCDIS)))\\nprint(\"TT_df elastic: \" + str(len(reindex_TT_df_NueEElastic)))\\nprint(\"y_full elastic: \" + str(len(reindex_y_full_NueEElastic)))\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "print(\"Before Reduction  :\")\n",
    "print(\"TT_df inelastic: \" + str(len(reindex_TT_df_CCDIS)))\n",
    "print(\"y_full inelastic: \" + str(len(reindex_y_full_CCDIS)))\n",
    "print(\"TT_df elastic: \" + str(len(reindex_TT_df_NueEElastic)))\n",
    "print(\"y_full elastic: \" + str(len(reindex_y_full_NueEElastic)))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting events to ensure equal number of elastic and inelastic events\n",
    "event_limit = min(len(reindex_TT_df_CCDIS),len(reindex_TT_df_NueEElastic))\n",
    "\n",
    "remove = int(len(reindex_TT_df_CCDIS)-event_limit)+1\n",
    "reindex_TT_df_CCDIS = reindex_TT_df_CCDIS[:-remove]\n",
    "reindex_y_full_CCDIS = reindex_y_full_CCDIS[:-remove]\n",
    "\n",
    "remove = int(len(reindex_TT_df_NueEElastic)-event_limit)+1\n",
    "reindex_TT_df_NueEElastic = reindex_TT_df_NueEElastic[:-remove]\n",
    "reindex_y_full_NueEElastic = reindex_y_full_NueEElastic[:-remove]\n",
    "\n",
    "# Merging CCDIS and NueEElastic in a single array\n",
    "reindex_TT_df = pd.concat([reindex_TT_df_CCDIS,reindex_TT_df_NueEElastic], ignore_index=True)\n",
    "reindex_y_full = pd.concat([reindex_y_full_CCDIS,reindex_y_full_NueEElastic], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# -1 for CCDIS\\nfor i in range(event_limit-1):\\n    reindex_y_full.iloc[i][\"Label\"] = -1  \\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "print(\"After Reduction  :\")\n",
    "print(\"TT_df inelastic: \" + str(len(reindex_TT_df_CCDIS)))\n",
    "print(\"y_full inelastic: \" + str(len(reindex_y_full_CCDIS)))\n",
    "print(\"TT_df elastic: \" + str(len(reindex_TT_df_NueEElastic)))\n",
    "print(\"y_full elastic: \" + str(len(reindex_y_full_NueEElastic)))\n",
    "\n",
    "print(\"Combined TT_df : \" + str(len(reindex_TT_df)))\n",
    "print(\"Combined y_full: \" + str(len(reindex_y_full)))\n",
    "'''\n",
    "'''\n",
    "# -1 for CCDIS\n",
    "for i in range(event_limit-1):\n",
    "    reindex_y_full.iloc[i][\"Label\"] = -1  \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Splitting the data into a training and a testing sample\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fsergeev/anaconda3/envs/ship_tt/lib/python3.7/site-packages/ipykernel_launcher.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "# reset to empty space\n",
    "chunklist_TT_df_NueEElastic = []\n",
    "chunklist_y_full_NueEElastic = []\n",
    "reindex_TT_df_NueEElastic = []\n",
    "reindex_y_full_NueEElastic = []\n",
    "\n",
    "chunklist_TT_df_CCDIS = []\n",
    "chunklist_y_full_CCDIS = []\n",
    "reindex_TT_df_CCDIS = []\n",
    "reindex_y_full_CCDIS = []\n",
    "\n",
    "\n",
    "nb_of_plane = len(params.snd_params[params.configuration][\"TT_POSITIONS\"])\n",
    "\n",
    "# True value of NRJ for each true Nue event\n",
    "y = reindex_y_full[[\"E\"]]\n",
    "NORM = 1. / 4000\n",
    "y[\"E\"] *= NORM\n",
    "\n",
    "# Spliting\n",
    "print(\"\\nSplitting the data into a training and a testing sample\")\n",
    "\n",
    "indeces = np.arange(len(reindex_TT_df))\n",
    "train_indeces, test_indeces, _, _ = train_test_split(indeces, indeces, train_size=0.9, random_state=1543)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get smaller datasets to make training quicker\n",
    "\n",
    "debug_coef = 0.2\n",
    "\n",
    "train_indeces = train_indeces[0 : int(train_indeces.shape[0] * debug_coef)]\n",
    "test_indeces  = train_indeces[0 : int(test_indeces .shape[0] * debug_coef)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#batch_size = 512\n",
    "#batch_size = 128\n",
    "batch_size = 16\n",
    "\n",
    "train_dataset = MyDataset(reindex_TT_df, y, params, train_indeces, n_filters=nb_of_plane)\n",
    "train_batch_gen = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "\n",
    "test_dataset = MyDataset(reindex_TT_df, y, params, test_indeces, n_filters=nb_of_plane)\n",
    "test_batch_gen = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "# reset to empty space\n",
    "reindex_TT_df=[]\n",
    "\n",
    "# Saving the true Energy for the test sample\n",
    "TrueE_test=y[\"E\"][test_indeces]\n",
    "np.save(\"results/TrueE_test.npy\",TrueE_test)\n",
    "\n",
    "# Saving the Label for the test sample\n",
    "TrueLabel_test = reindex_y_full[\"Label\"][test_indeces]\n",
    "np.save(\"results/TrueLabel_test.npy\",TrueLabel_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 201, 201])\n"
     ]
    }
   ],
   "source": [
    "for X_batch, y_batch in train_batch_gen:\n",
    "    print(X_batch[0].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_default_tensor_type(\"torch.cuda.FloatTensor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘bayesian_results/9X0_file’: File exists\n"
     ]
    }
   ],
   "source": [
    "# Creating the network\n",
    "net = BNN(n_input_filters=nb_of_plane)\n",
    "guide = AutoDiagonalNormal(net)\n",
    "predictive = Predictive(net, guide=guide, num_samples=100, return_sites=(\"obs\", \"_RETURN\"))\n",
    "\n",
    "# Loose rate, num epoch and weight decay parameters of our network backprop actions\n",
    "adam = pyro.optim.Adam({\"lr\": 1e-3})\n",
    "svi = SVI(net, guide, adam, loss=Trace_ELBO()) # stochastic variational inference class\n",
    "\n",
    "num_epochs = 3\n",
    "\n",
    "train_loss = []\n",
    "val_accuracy_1 = []\n",
    "val_accuracy_2 = []\n",
    "\n",
    "# Create a directory where to store the 9X0 files\n",
    "os.system(\"mkdir bayesian_results/9X0_file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Now Trainig the network:\n"
     ]
    }
   ],
   "source": [
    "#Training\n",
    "print(\"\\nNow Trainig the network:\")\n",
    "# Create a .txt file where we will store some info for graphs\n",
    "f=open(\"bayesian_results/NN_performance.txt\",\"a\")\n",
    "f.write(\"Epoch/Time it took (s)/Loss/Validation energy (%)/Validation distance (%)\\n\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Logger(object):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def plot_losses(self, epoch, num_epochs, start_time):\n",
    "        # Print and save in NN_performance.txt the results for this epoch:\n",
    "        print(\"Epoch {} of {} took {:.3f}s\".format(epoch + 1, num_epochs, time.time() - start_time))\n",
    "        print(\"  training loss (in-iteration): \\t{:.6f}\".format(train_loss[-1]))\n",
    "        print(\"  validation Energy:\\t\\t{:.4f} %\".format(val_accuracy_1[-1]))\n",
    "        #print(\"  validation distance:\\t\\t{:.4f} %\".format(val_accuracy_2[-1]))\n",
    "\n",
    "        f=open(\"bayesian_results/NN_performance.txt\",\"a\")\n",
    "        f.write(\"{};{:.3f};\".format(epoch + 1, time.time() - start_time))\n",
    "        f.write(\"\\t{:.6f};\".format(train_loss[-1]))\n",
    "        f.write(\"\\t\\t{:.4f}\\n\".format(val_accuracy_1[-1]))\n",
    "        #f.write(\"\\t\\t{:.4f}\\n\".format(val_accuracy_2[-1]))\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summary(samples):\n",
    "    site_stats = {}\n",
    "    for k, v in samples.items():\n",
    "        site_stats[k] = {\n",
    "            \"mean\": torch.mean(v, 0),\n",
    "            \"std\":  torch.std(v, 0),\n",
    "            \"5%\":   v.kthvalue(int(len(v) * 0.05), dim=0)[0],\n",
    "            \"95%\":  v.kthvalue(int(len(v) * 0.95), dim=0)[0],\n",
    "        }\n",
    "    return site_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training(lr, num_epochs, opt):\n",
    "    try:\n",
    "        pyro.clear_param_store()\n",
    "\n",
    "        total = int(len(train_indeces) / batch_size)\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            # In each epoch, we do a full pass over the training data:\n",
    "            start_time = time.time()\n",
    "            \n",
    "            guide.requires_grad_(True)\n",
    "            \n",
    "            epoch_loss = 0\n",
    "            for X_batch, y_batch in tqdm(train_batch_gen, total=total):\n",
    "                loss = svi.step(X_batch.to(device), y_batch.to(device))\n",
    "                epoch_loss += loss\n",
    "\n",
    "            train_loss.append(epoch_loss / (len(train_indeces) // batch_size + 1))\n",
    "\n",
    "            guide.requires_grad_(False)\n",
    "\n",
    "            y_score = []\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for (X_batch, y_batch) in tqdm(test_batch_gen, total = int(len(test_indeces) / batch_size)):\n",
    "                    samples = predictive(X_batch.to(device), y_batch.to(device))\n",
    "                    pred_summary = summary(samples)\n",
    "                    bayes_preds = pred_summary[\"_RETURN\"]['mean']\n",
    "                    \n",
    "                    y_pred = bayes_preds.cpu().detach().numpy()\n",
    "                    y_score.extend(y_pred)\n",
    "\n",
    "            y_score = mean_squared_error(y.iloc[test_indeces], np.asarray(y_score), multioutput='raw_values')\n",
    "            val_accuracy_1.append(y_score[0])\n",
    "            #val_accuracy_2.append(y_score[1])    \n",
    "            \n",
    "            # Visualize\n",
    "            display.clear_output(wait=True)\n",
    "            logger.plot_losses(epoch, num_epochs, start_time)\n",
    "\n",
    "            #Saving network for each 1 epoch\n",
    "            if (epoch + 1) % 1 == 0:\n",
    "                with open(\"bayesian_results/9X0_file/\" + str(epoch) + \"_9X0_coordconv.pt\", 'wb') as f:\n",
    "                    torch.save(net, f)       \n",
    "                # optimizer is inside svi\n",
    "                # lr = lr / 2\n",
    "                # opt = torch.optim.Adam(net.model.parameters(), lr=lr)\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 of 3 took 157.573s\n",
      "  training loss (in-iteration): \t1888.426353\n",
      "  validation Energy:\t\t7.1178 %\n"
     ]
    }
   ],
   "source": [
    "logger = Logger()\n",
    "run_training(lr=None, num_epochs=num_epochs, opt=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 83/83 [01:36<00:00,  1.16s/it]\n",
      "  0%|          | 0/83 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save Prediction for epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 83/83 [01:35<00:00,  1.15s/it]\n",
      "  0%|          | 0/83 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save Prediction for epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 83/83 [01:36<00:00,  1.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save Prediction for epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "mkdir: cannot create directory ‘bayesian_results/PredE_file’: File exists\n"
     ]
    }
   ],
   "source": [
    "# Saving the prediction at each epoch\n",
    "\n",
    "# Create a directory where to store the prediction files\n",
    "os.system(\"mkdir bayesian_results/PredE_file\")\n",
    "\n",
    "for i in [0,1,2]:#[9, 19, 29, 39, 49]:\n",
    "    net = torch.load(\"bayesian_results/9X0_file/\" + str(i) + \"_9X0_coordconv.pt\")\n",
    "    preds = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for (X_batch, y_batch) in tqdm(test_batch_gen):\n",
    "            samples = predictive(X_batch.to(device), y_batch.to(device))\n",
    "            pred_summary = summary(samples)\n",
    "            bayes_preds = pred_summary[\"_RETURN\"]['mean']\n",
    "\n",
    "            y_pred = bayes_preds.cpu().detach().numpy()\n",
    "            preds.append(y_pred)\n",
    "    \n",
    "    np.save(\"bayesian_results/PredE_file/\" + str(i) + \"_PredE_test.npy\", preds)\n",
    "    print(\"Save Prediction for epoch \"+ str(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
