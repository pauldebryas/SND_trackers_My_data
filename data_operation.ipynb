{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EM shower reconstruction at SND@LHC\n",
    "\n",
    "1. __make sure the preprocessing has already been done__\n",
    "\n",
    "2. __make sure `results` folder exists__\n",
    "\n",
    "https://arxiv.org/pdf/2002.08722.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to JupyROOT 6.18/00\n"
     ]
    }
   ],
   "source": [
    "# imports from utils.py & net.py\n",
    "from utils import DataPreprocess, Parameters\n",
    "#from net import SNDNet, BNN, MyDataset, digitize_signal, digitize_signal_1d\n",
    "\n",
    "# python\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "from matplotlib import pylab as plt\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from IPython import display\n",
    "\n",
    "# system\n",
    "import os\n",
    "import gc  # Gabage collector interface (to debug stuff)\n",
    "import sys\n",
    "\n",
    "# ml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# dl\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA devices available:\n",
      "\tQuadro RTX 4000\twith CUDA capability (7, 5)\n",
      "\tQuadro RTX 4000\twith CUDA capability (7, 5)\n"
     ]
    }
   ],
   "source": [
    "# Test to see if cuda is available or not + listed the CUDA devices that are available\n",
    "try:\n",
    "    assert(torch.cuda.is_available())\n",
    "except:\n",
    "    raise Exception(\"CUDA is not available\")\n",
    "    \n",
    "n_devices = torch.cuda.device_count()\n",
    "print(\"CUDA devices available:\")\n",
    "\n",
    "for i in range(n_devices):\n",
    "    print(\"\\t{}\\twith CUDA capability {}\".format(torch.cuda.get_device_name      (device=i), \n",
    "                                                 torch.cuda.get_device_capability(device=i)))\n",
    "\n",
    "device = torch.device(\"cuda\", 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn off interactive plotting: for long run it screws up everything\n",
    "plt.ioff()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "DETECTOR_PARAMS = Parameters(\"SNDatLHC\")\n",
    "DETECTOR_CONFIG = DETECTOR_PARAMS.snd_params[DETECTOR_PARAMS.configuration]\n",
    "\n",
    "# number of planes of the detector\n",
    "#NB_PLANE = dict()\n",
    "\n",
    "#NB_PLANE['scifi']   = len(DETECTOR_CONFIG['SciFi_tracker']        ['TT_POSITIONS'])\n",
    "#NB_PLANE['up_mu']   = len(DETECTOR_CONFIG['Mu_tracker_upstream']  ['TT_POSITIONS'])\n",
    "#NB_PLANE['down_mu'] = len(DETECTOR_CONFIG['Mu_tracker_downstream']['TT_POSITIONS'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data processing\n",
    "\n",
    "Here we load and process __pickle__ files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.process_pickle import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = dict()\n",
    "DATA_PATH['nuel']  = \"~/snd_data/nue\"\n",
    "DATA_PATH['numu']  = \"~/snd_data/numu\"\n",
    "DATA_PATH['nutau'] = \"~/snd_data/nutau\"\n",
    "\n",
    "EVENTS_PER_FILE = 4000 # todo -> read from the files ?\n",
    "FILES_NUM       = 8   # MAX=100 / todo -> read from directory ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scifi_arr, mu_arr, en_arr = load_dataframes(DETECTOR_PARAMS, \n",
    "#                                            DATA_PATH, EVENTS_PER_FILE, FILES_NUM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scifi_arr, mu_arr, en_arr = merge_events_arrays(scifi_arr, mu_arr, en_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#en_arr = normalise_target_energy(en_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation\n",
    "\n",
    "Here we prepare (load or, if needed, create) the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.operate_datasets import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create_dataset('true', DETECTOR_PARAMS, DATA_PATH, EVENTS_PER_FILE, FILES_NUM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create_dataset('sum', DETECTOR_PARAMS, DATA_PATH, EVENTS_PER_FILE, FILES_NUM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create_dataset('longitudal', DETECTOR_PARAMS, DATA_PATH, EVENTS_PER_FILE, FILES_NUM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading the tt_cleared.pkl & y_cleared.pkl files by chunk of CCDIS and NueEElastic\n",
      "Before Reduction (file ~/snd_data/nue):\n",
      "  TT_df  : 32000\n",
      "  MU_df  : 32000\n",
      "  y_full : 32000\n",
      "Before Reduction (file ~/snd_data/numu):\n",
      "  TT_df  : 32000\n",
      "  MU_df  : 32000\n",
      "  y_full : 32000\n",
      "Before Reduction (file ~/snd_data/nutau):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 45/96000 [00:00<03:59, 401.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  TT_df  : 32000\n",
      "  MU_df  : 32000\n",
      "  y_full : 32000\n",
      "After Reduction  :\n",
      "\n",
      "Particle type: nuel\n",
      "  scifi_arr : 32000\n",
      "  mu_arr    : 32000\n",
      "  en_arr    : 32000\n",
      "Particle type: numu\n",
      "  scifi_arr : 32000\n",
      "  mu_arr    : 32000\n",
      "  en_arr    : 32000\n",
      "Particle type: nutau\n",
      "  scifi_arr : 32000\n",
      "  mu_arr    : 32000\n",
      "  en_arr    : 32000\n",
      "\n",
      "combined_scifi_arr : 96000\n",
      "combined_mu_arr: 96000\n",
      "combined_en_arr: 96000\n",
      "projection\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 96000/96000 [02:05<00:00, 763.21it/s] \n"
     ]
    }
   ],
   "source": [
    "create_dataset('projection', DETECTOR_PARAMS, DATA_PATH, EVENTS_PER_FILE, FILES_NUM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# memory troubles!\n",
    "# be very carefull when using this\n",
    "### create_dataset('plane', DETECTOR_PARAMS, DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mv *.npz ../snd_data/d_data/new_dataset/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfull_X, full_y = load_dataset('~/snd_data/new_dataset/', 'longitudal')\\n\\nX_train, y_train, _, _ = split_dataset(full_X, full_y)\\n# min_clip = 25\\n# X_train, y_train = clip_dataset(X_train, y_train, min_clip)\\n\\n\\nfrom sklearn import svm\\n\\nreg_svr = svm.SVR(gamma='scale')\\n#reg_svr = svm.LinearSVR(max_iter=10**5)\\n\\nreg_svr.fit(X_train, y_train)\\n\\nscore_svr = reg_svr.score(X_train, y_train)\\n\\nprint('SVM: ', score_svr)\\n\\ny_pred_svr = reg_svr.predict(X_train)\\n\\nX_sum = X_train.sum(axis=1).reshape(-1,1)\\ny_sum = y_train.reshape(-1,1)\\ny_pred_svr = y_pred_svr.reshape(-1,1)\\n\\n\\nplot_res_vs_energy(X_sum, y_sum, y_pred_svr) \\nplot_res_hist(y_sum, y_pred_svr)\\nplot_2d_energy_hist(X_sum, y_sum, y_pred_svr)\\nget_scores(y_sum, y_pred_svr)\\n\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Support Vector Regression (SVR)\n",
    "\n",
    "'''\n",
    "full_X, full_y = load_dataset('~/snd_data/new_dataset/', 'longitudal')\n",
    "\n",
    "X_train, y_train, _, _ = split_dataset(full_X, full_y)\n",
    "# min_clip = 25\n",
    "# X_train, y_train = clip_dataset(X_train, y_train, min_clip)\n",
    "\n",
    "\n",
    "from sklearn import svm\n",
    "\n",
    "reg_svr = svm.SVR(gamma='scale')\n",
    "#reg_svr = svm.LinearSVR(max_iter=10**5)\n",
    "\n",
    "reg_svr.fit(X_train, y_train)\n",
    "\n",
    "score_svr = reg_svr.score(X_train, y_train)\n",
    "\n",
    "print('SVM: ', score_svr)\n",
    "\n",
    "y_pred_svr = reg_svr.predict(X_train)\n",
    "\n",
    "X_sum = X_train.sum(axis=1).reshape(-1,1)\n",
    "y_sum = y_train.reshape(-1,1)\n",
    "y_pred_svr = y_pred_svr.reshape(-1,1)\n",
    "\n",
    "\n",
    "plot_res_vs_energy(X_sum, y_sum, y_pred_svr) \n",
    "plot_res_hist(y_sum, y_pred_svr)\n",
    "plot_2d_energy_hist(X_sum, y_sum, y_pred_svr)\n",
    "get_scores(y_sum, y_pred_svr)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook regression_full_sum.ipynb to notebook\n",
      "[NbConvertApp] Executing notebook with kernel: python3\n",
      "[NbConvertApp] Writing 168125 bytes to regression_full_sum.ipynb\n",
      "[NbConvertApp] Converting notebook regression_plane_sum.ipynb to notebook\n",
      "[NbConvertApp] Executing notebook with kernel: python3\n",
      "[NbConvertApp] Writing 231943 bytes to regression_plane_sum.ipynb\n",
      "[NbConvertApp] Converting notebook regression_projections.ipynb to notebook\n",
      "[NbConvertApp] Executing notebook with kernel: python3\n",
      "[NbConvertApp] Writing 235004 bytes to regression_projections.ipynb\n",
      "[NbConvertApp] Converting notebook regression_true_hits.ipynb to notebook\n",
      "[NbConvertApp] Executing notebook with kernel: python3\n",
      "[NbConvertApp] Writing 218294 bytes to regression_true_hits.ipynb\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert --to notebook --inplace --execute regression_*.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook nn_full_sum.ipynb to notebook\n",
      "[NbConvertApp] Executing notebook with kernel: python3\n",
      "[NbConvertApp] Writing 196209 bytes to nn_full_sum.ipynb\n",
      "[NbConvertApp] Converting notebook nn_plane_sum.ipynb to notebook\n",
      "[NbConvertApp] Executing notebook with kernel: python3\n",
      "[NbConvertApp] Writing 274110 bytes to nn_plane_sum.ipynb\n",
      "[NbConvertApp] Converting notebook nn_projections.ipynb to notebook\n",
      "[NbConvertApp] Executing notebook with kernel: python3\n",
      "[NbConvertApp] Writing 246637 bytes to nn_projections.ipynb\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert --to notebook --inplace --execute nn_*.ipynb --ExecutePreprocessor.timeout=180"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook bnn_full_sum.ipynb to notebook\n",
      "[NbConvertApp] Executing notebook with kernel: python3\n",
      "[NbConvertApp] Writing 240058 bytes to bnn_full_sum.ipynb\n",
      "[NbConvertApp] Converting notebook bnn_plane_sum.ipynb to notebook\n",
      "[NbConvertApp] Executing notebook with kernel: python3\n",
      "[NbConvertApp] Writing 248118 bytes to bnn_plane_sum.ipynb\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert --to notebook --inplace --execute bnn_*.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.model_evaluation import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def present_scores(df):\n",
    "    df = df.reindex(sorted(df.columns), axis=1)\n",
    "    first_col = df.pop('Score')\n",
    "    df.insert(0, 'Score', first_col)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Score</th>\n",
       "      <th>BNN-on-full-sum</th>\n",
       "      <th>BNN-on-plane-sums</th>\n",
       "      <th>L2-on-full-sum</th>\n",
       "      <th>L2-on-plane-sums</th>\n",
       "      <th>L2-on-projections</th>\n",
       "      <th>L2-on-true-hits</th>\n",
       "      <th>NN-on-full-sum</th>\n",
       "      <th>NN-on-plane-sums</th>\n",
       "      <th>NN-on-projections</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>explained_variance_score</td>\n",
       "      <td>-6.614871</td>\n",
       "      <td>-0.075949</td>\n",
       "      <td>0.013989</td>\n",
       "      <td>0.279462</td>\n",
       "      <td>0.352173</td>\n",
       "      <td>0.127250</td>\n",
       "      <td>0.040829</td>\n",
       "      <td>0.177364</td>\n",
       "      <td>0.366390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>max_error</td>\n",
       "      <td>15.184106</td>\n",
       "      <td>3.905588</td>\n",
       "      <td>0.962614</td>\n",
       "      <td>1.035705</td>\n",
       "      <td>1.083276</td>\n",
       "      <td>0.999070</td>\n",
       "      <td>1.035946</td>\n",
       "      <td>2.611687</td>\n",
       "      <td>1.052366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>mean_absolute_error</td>\n",
       "      <td>0.227494</td>\n",
       "      <td>0.133608</td>\n",
       "      <td>0.140651</td>\n",
       "      <td>0.113108</td>\n",
       "      <td>0.107232</td>\n",
       "      <td>0.125568</td>\n",
       "      <td>0.127268</td>\n",
       "      <td>0.117873</td>\n",
       "      <td>0.106135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>mean_squared_error</td>\n",
       "      <td>0.244339</td>\n",
       "      <td>0.034678</td>\n",
       "      <td>0.031907</td>\n",
       "      <td>0.023052</td>\n",
       "      <td>0.020750</td>\n",
       "      <td>0.027922</td>\n",
       "      <td>0.031396</td>\n",
       "      <td>0.026318</td>\n",
       "      <td>0.020295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>median_absolute_error</td>\n",
       "      <td>0.131900</td>\n",
       "      <td>0.108528</td>\n",
       "      <td>0.127065</td>\n",
       "      <td>0.092548</td>\n",
       "      <td>0.087339</td>\n",
       "      <td>0.107848</td>\n",
       "      <td>0.099604</td>\n",
       "      <td>0.094789</td>\n",
       "      <td>0.087308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>r2_score</td>\n",
       "      <td>-6.637339</td>\n",
       "      <td>-0.083925</td>\n",
       "      <td>0.002681</td>\n",
       "      <td>0.279462</td>\n",
       "      <td>0.352172</td>\n",
       "      <td>0.127250</td>\n",
       "      <td>0.018643</td>\n",
       "      <td>0.177362</td>\n",
       "      <td>0.366379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>avg_resolution</td>\n",
       "      <td>1.120973</td>\n",
       "      <td>1.818890</td>\n",
       "      <td>2.375080</td>\n",
       "      <td>1.498581</td>\n",
       "      <td>1.400614</td>\n",
       "      <td>1.872374</td>\n",
       "      <td>1.551465</td>\n",
       "      <td>1.489076</td>\n",
       "      <td>1.400872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>std_resolution</td>\n",
       "      <td>8.198869</td>\n",
       "      <td>7.505279</td>\n",
       "      <td>8.835667</td>\n",
       "      <td>5.833365</td>\n",
       "      <td>5.602714</td>\n",
       "      <td>7.319886</td>\n",
       "      <td>6.081390</td>\n",
       "      <td>5.644837</td>\n",
       "      <td>5.425789</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Score  BNN-on-full-sum  BNN-on-plane-sums  \\\n",
       "0  explained_variance_score        -6.614871          -0.075949   \n",
       "1                 max_error        15.184106           3.905588   \n",
       "2       mean_absolute_error         0.227494           0.133608   \n",
       "3        mean_squared_error         0.244339           0.034678   \n",
       "4     median_absolute_error         0.131900           0.108528   \n",
       "5                  r2_score        -6.637339          -0.083925   \n",
       "6            avg_resolution         1.120973           1.818890   \n",
       "7            std_resolution         8.198869           7.505279   \n",
       "\n",
       "   L2-on-full-sum  L2-on-plane-sums  L2-on-projections  L2-on-true-hits  \\\n",
       "0        0.013989          0.279462           0.352173         0.127250   \n",
       "1        0.962614          1.035705           1.083276         0.999070   \n",
       "2        0.140651          0.113108           0.107232         0.125568   \n",
       "3        0.031907          0.023052           0.020750         0.027922   \n",
       "4        0.127065          0.092548           0.087339         0.107848   \n",
       "5        0.002681          0.279462           0.352172         0.127250   \n",
       "6        2.375080          1.498581           1.400614         1.872374   \n",
       "7        8.835667          5.833365           5.602714         7.319886   \n",
       "\n",
       "   NN-on-full-sum  NN-on-plane-sums  NN-on-projections  \n",
       "0        0.040829          0.177364           0.366390  \n",
       "1        1.035946          2.611687           1.052366  \n",
       "2        0.127268          0.117873           0.106135  \n",
       "3        0.031396          0.026318           0.020295  \n",
       "4        0.099604          0.094789           0.087308  \n",
       "5        0.018643          0.177362           0.366379  \n",
       "6        1.551465          1.489076           1.400872  \n",
       "7        6.081390          5.644837           5.425789  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "present_scores(collect_all_scores())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Score</th>\n",
       "      <th>BNN-on-full-sum</th>\n",
       "      <th>BNN-on-plane-sums</th>\n",
       "      <th>L2-on-full-sum</th>\n",
       "      <th>L2-on-plane-sums</th>\n",
       "      <th>L2-on-projections</th>\n",
       "      <th>L2-on-true-hits</th>\n",
       "      <th>NN-on-full-sum</th>\n",
       "      <th>NN-on-plane-sums</th>\n",
       "      <th>NN-on-projections</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>explained_variance_score</td>\n",
       "      <td>-3.174983</td>\n",
       "      <td>-0.071997</td>\n",
       "      <td>0.015207</td>\n",
       "      <td>0.277664</td>\n",
       "      <td>0.337756</td>\n",
       "      <td>0.130124</td>\n",
       "      <td>0.045816</td>\n",
       "      <td>0.123731</td>\n",
       "      <td>0.355991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>max_error</td>\n",
       "      <td>6.641919</td>\n",
       "      <td>2.012484</td>\n",
       "      <td>0.907420</td>\n",
       "      <td>0.856899</td>\n",
       "      <td>0.805914</td>\n",
       "      <td>0.894357</td>\n",
       "      <td>0.943592</td>\n",
       "      <td>3.245611</td>\n",
       "      <td>0.933178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>mean_absolute_error</td>\n",
       "      <td>0.212279</td>\n",
       "      <td>0.134610</td>\n",
       "      <td>0.140498</td>\n",
       "      <td>0.113324</td>\n",
       "      <td>0.107791</td>\n",
       "      <td>0.125348</td>\n",
       "      <td>0.126719</td>\n",
       "      <td>0.118628</td>\n",
       "      <td>0.106765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>mean_squared_error</td>\n",
       "      <td>0.137757</td>\n",
       "      <td>0.034476</td>\n",
       "      <td>0.031751</td>\n",
       "      <td>0.023037</td>\n",
       "      <td>0.020905</td>\n",
       "      <td>0.027743</td>\n",
       "      <td>0.031179</td>\n",
       "      <td>0.027947</td>\n",
       "      <td>0.020337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>median_absolute_error</td>\n",
       "      <td>0.139006</td>\n",
       "      <td>0.109266</td>\n",
       "      <td>0.126730</td>\n",
       "      <td>0.092459</td>\n",
       "      <td>0.087961</td>\n",
       "      <td>0.107367</td>\n",
       "      <td>0.098393</td>\n",
       "      <td>0.095027</td>\n",
       "      <td>0.087584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>r2_score</td>\n",
       "      <td>-3.319428</td>\n",
       "      <td>-0.081010</td>\n",
       "      <td>0.004419</td>\n",
       "      <td>0.277664</td>\n",
       "      <td>0.337663</td>\n",
       "      <td>0.130113</td>\n",
       "      <td>0.022369</td>\n",
       "      <td>0.123714</td>\n",
       "      <td>0.355646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>avg_resolution</td>\n",
       "      <td>0.723605</td>\n",
       "      <td>1.724431</td>\n",
       "      <td>2.247229</td>\n",
       "      <td>1.406165</td>\n",
       "      <td>1.324153</td>\n",
       "      <td>1.769530</td>\n",
       "      <td>1.460338</td>\n",
       "      <td>1.406977</td>\n",
       "      <td>1.341442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>std_resolution</td>\n",
       "      <td>5.612163</td>\n",
       "      <td>5.691439</td>\n",
       "      <td>6.652317</td>\n",
       "      <td>4.396246</td>\n",
       "      <td>4.061178</td>\n",
       "      <td>5.507832</td>\n",
       "      <td>4.717037</td>\n",
       "      <td>4.327076</td>\n",
       "      <td>4.023060</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Score  BNN-on-full-sum  BNN-on-plane-sums  \\\n",
       "0  explained_variance_score        -3.174983          -0.071997   \n",
       "1                 max_error         6.641919           2.012484   \n",
       "2       mean_absolute_error         0.212279           0.134610   \n",
       "3        mean_squared_error         0.137757           0.034476   \n",
       "4     median_absolute_error         0.139006           0.109266   \n",
       "5                  r2_score        -3.319428          -0.081010   \n",
       "6            avg_resolution         0.723605           1.724431   \n",
       "7            std_resolution         5.612163           5.691439   \n",
       "\n",
       "   L2-on-full-sum  L2-on-plane-sums  L2-on-projections  L2-on-true-hits  \\\n",
       "0        0.015207          0.277664           0.337756         0.130124   \n",
       "1        0.907420          0.856899           0.805914         0.894357   \n",
       "2        0.140498          0.113324           0.107791         0.125348   \n",
       "3        0.031751          0.023037           0.020905         0.027743   \n",
       "4        0.126730          0.092459           0.087961         0.107367   \n",
       "5        0.004419          0.277664           0.337663         0.130113   \n",
       "6        2.247229          1.406165           1.324153         1.769530   \n",
       "7        6.652317          4.396246           4.061178         5.507832   \n",
       "\n",
       "   NN-on-full-sum  NN-on-plane-sums  NN-on-projections  \n",
       "0        0.045816          0.123731           0.355991  \n",
       "1        0.943592          3.245611           0.933178  \n",
       "2        0.126719          0.118628           0.106765  \n",
       "3        0.031179          0.027947           0.020337  \n",
       "4        0.098393          0.095027           0.087584  \n",
       "5        0.022369          0.123714           0.355646  \n",
       "6        1.460338          1.406977           1.341442  \n",
       "7        4.717037          4.327076           4.023060  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "present_scores(collect_all_scores(model_evaluation.TEST_SCORES_DIR))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "from pyro.nn import PyroModule, PyroSample\n",
    "from pyro.infer.autoguide import AutoDiagonalNormal\n",
    "from pyro.infer import SVI, Trace_ELBO, Predictive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset_split' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-8d1cf0be9dce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmin_clip\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m25\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfull_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset_clip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_clip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dataset_split' is not defined"
     ]
    }
   ],
   "source": [
    "full_X, full_y = load_dataset('~/snd_data/new_dataset/', 'sum')\n",
    "min_clip = 25\n",
    "\n",
    "X_train, y_train = dataset_split(full_X, full_y)\n",
    "X_train, y_train = dataset_clip(X_train, y_train, min_clip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.tensor(X_train).float().reshape(-1,1)\n",
    "y_train = torch.tensor(y_train).float().reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyro.distributions import constraints\n",
    "\n",
    "\n",
    "class BayesianRegression(PyroModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = PyroModule[nn.Linear](1, 1)\n",
    "        \n",
    "        # prior over parameters\n",
    "        self.linear.weight = PyroSample(dist.Normal(1e-4, 5e-5).expand([1, 1]).to_event(2))\n",
    "        self.linear.bias   = PyroSample(dist.Normal(1e-1, 5e-2).expand([1]).to_event(1))\n",
    "        return\n",
    "    \n",
    "    def forward(self, x, y=None):\n",
    "        # energy (from the model)\n",
    "        mean = self.linear(x).squeeze(-1)\n",
    "        \n",
    "        # noise (learnable)\n",
    "        sigma = pyro.sample(\"sigma\", dist.Uniform(0., 1.)) \n",
    "        \n",
    "        with pyro.plate(\"data\", x.shape[0]):\n",
    "            obs = pyro.sample(\"obs\", dist.Normal(mean, sigma), obs=y)\n",
    "        \n",
    "        return mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(x, y=None):\n",
    "    bias   = pyro.sample(\"bias\",   dist.Normal(1e-4, 1e-4))\n",
    "    weight = pyro.sample(\"weight\", dist.Normal(0.1, 0.1))\n",
    "\n",
    "    mean = bias + weight * x # + mu_weight * muon_hits\n",
    "    mean = mean.squeeze(-1)\n",
    "    sigma = pyro.sample(\"sigma\", dist.Uniform(0., 0.1)) \n",
    "\n",
    "    with pyro.plate(\"data\", x.shape[0]):\n",
    "        pyro.sample(\"obs\", dist.Normal(mean, sigma), obs=y)\n",
    "\n",
    "    return mean\n",
    "\n",
    "\n",
    "def guide(x, y=None):\n",
    "    b_loc   = pyro.param(\"b_loc\",   torch.tensor(1e-4), constraint=constraints.positive)\n",
    "    b_scale = pyro.param(\"b_scale\", torch.tensor(1e-4), constraint=constraints.positive)\n",
    "\n",
    "    w_loc   = pyro.param(\"w_loc\",   torch.tensor(0.1), constraint=constraints.positive)\n",
    "    w_scale = pyro.param(\"w_scale\", torch.tensor(0.1), constraint=constraints.positive)\n",
    "\n",
    "    bias =   pyro.sample(\"bias\",   dist.Normal(b_loc, b_scale))\n",
    "    weight = pyro.sample(\"weight\", dist.Normal(w_loc, w_scale))\n",
    "\n",
    "    #sigma_loc = pyro.param('sigma_loc', torch.tensor(5.))\n",
    "    sigma = pyro.sample(\"sigma\", dist.Normal(0.05, 0.005))\n",
    "\n",
    "    mean = bias + weight * x\n",
    "    \n",
    "    return mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reg_model = model # BayesianRegression()\n",
    "reg_guide = guide # AutoDiagonalNormal(model)\n",
    "\n",
    "num_steps = 50\n",
    "initial_lr = 1.0\n",
    "gamma = 0.5  # final learning rate will be gamma * initial_lr\n",
    "lrd = gamma ** (1 / num_steps)\n",
    "\n",
    "adam = pyro.optim.ClippedAdam({\"lr\": initial_lr, 'lrd': lrd})\n",
    "\n",
    "#adam = pyro.optim.ClippedAdam({\"lr\": 1.0, \"lrd\": 0.5})\n",
    "svi = SVI(reg_model, reg_guide, adam, loss=Trace_ELBO())\n",
    "num_iterations = 200\n",
    "\n",
    "pyro.clear_param_store()\n",
    "loss_arr = []\n",
    "\n",
    "\n",
    "for j in range(num_iterations):\n",
    "    loss = svi.step(X_train, y_train)\n",
    "\n",
    "    loss_arr.append(loss)\n",
    "        \n",
    "    if j % 25 == 0:\n",
    "        print(\"[iteration %04d] loss: %.4f\" % (j + 1, loss / len(X_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_arr)\n",
    "plt.yscale('log')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('ELBO loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#guide.requires_grad_(False)\n",
    "\n",
    "for name, value in pyro.get_param_store().items():\n",
    "    print(name, pyro.param(name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summary(samples):\n",
    "    site_stats = {}\n",
    "    for site_name, values in samples.items():\n",
    "        site_stats[site_name] = {\n",
    "            \"mean\": torch.mean(values, 0),\n",
    "            \"std\" : torch.std (values, 0),\n",
    "            \"5%\"  : values.kthvalue(int(len(values) * 0.05), dim=0)[0],\n",
    "            \"95%\" : values.kthvalue(int(len(values) * 0.95), dim=0)[0],\n",
    "        }\n",
    "        \n",
    "    return site_stats\n",
    "\n",
    "\n",
    "predictive = Predictive(model, guide=guide, num_samples=800)\n",
    "samples = predictive(X_train)\n",
    "pred_summary = summary(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = pred_summary[\"obs\"]\n",
    "y = pred_summary[\"obs\"]\n",
    "predictions = pd.DataFrame({\n",
    "    \"mu_mean\": mu[\"mean\"],\n",
    "    \"mu_perc_5\": mu[\"5%\"],\n",
    "    \"mu_perc_95\": mu[\"95%\"],\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(X_train[:,0], predictions['mu_mean'].to_numpy(), 'o')\n",
    "plt.plot(X_train[:,0], predictions['mu_perc_5'].to_numpy(), 'o')\n",
    "plt.plot(X_train[:,0], predictions['mu_perc_95'].to_numpy(), 'o')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions['mu_mean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_2d_energy_hist(X_arr, y_true, predictions):\n",
    "    fig, ax = plt.subplots(figsize=(8,6))\n",
    "\n",
    "    y_pred_min  = predictions[\"mu_perc_5\"].to_numpy()\n",
    "    y_pred_mean = predictions[\"mu_mean\"]  .to_numpy()\n",
    "    y_pred_max  = predictions[\"mu_perc_95\"].to_numpy()\n",
    "    \n",
    "    print(X_arr.shape)\n",
    "    print(y_true.shape)\n",
    "    \n",
    "    hist = ax.hist2d(X_arr[:,0].numpy(), y_true.numpy(), \n",
    "                     bins=100, norm=mpl.colors.LogNorm(), vmax=150)\n",
    "    \n",
    "    plt.ylim(0)\n",
    "    plt.xlabel('pixel sum')\n",
    "    plt.ylabel('normalised energy')\n",
    "\n",
    "    #plt.axvline(x=min_clip, c='m', alpha=0.9, label='Min clip ' + str(min_clip))\n",
    "    ax.plot(X_arr[:, 0], y_pred_mean, 'g.', alpha=0.3, label='L2 w. clipped data')\n",
    "    ax.plot(X_arr[:, 0], y_pred_min, 'r.', marker='.', alpha=0.3, label='5%')\n",
    "    ax.plot(X_arr[:, 0], y_pred_max, 'b.', marker='.', alpha=0.3, label='95%')\n",
    "    #ax.fill_between(X_arr[:, 0], y_pred_min, y_pred_max,  alpha=0.3, color='deeppink')\n",
    "\n",
    "    #cbar = fig.colorbar(hist[3], ax=ax)\n",
    "    #|cbar.set_label('# of particles')\n",
    "\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_2d_energy_hist(X_train, y_train, predictions) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
